# Define LSTM Autoencoder model
from tensorflow.keras import Sequential
from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam

num_features = len(feature_cols)

def build_lstm_autoencoder(sequence_length: int, num_features: int, latent_dim: int = 64, dropout_rate: float = 0.2) -> tf.keras.Model:
    model = Sequential([
        Input(shape=(sequence_length, num_features)),
        LSTM(latent_dim, activation="tanh", return_sequences=False),
        Dropout(dropout_rate),
        RepeatVector(sequence_length),
        LSTM(latent_dim, activation="tanh", return_sequences=True),
        Dropout(dropout_rate),
        TimeDistributed(Dense(num_features)),
    ])
    model.compile(optimizer=Adam(learning_rate=1e-3), loss="mse")
    return model

autoencoder = build_lstm_autoencoder(sequence_length, num_features, latent_dim=64, dropout_rate=0.2)
autoencoder.summary()# Normalize features and create fixed-length sequences for one engine
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator

# Select one engine for simplicity
engine_id = 1
engine_df = fd001[fd001["engine_id"] == engine_id].sort_values("time").reset_index(drop=True)
print(f"Selected engine {engine_id} with {len(engine_df)} cycles")

feature_cols = [
    *[f"op_setting_{i}" for i in range(1, 4)],
    *[f"sensor_{i}" for i in range(1, 22)],
]

scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(engine_df[feature_cols].values)

sequence_length = 30
batch_size = 64

# TimeseriesGenerator yields (batch, sequence_length, features) and a target aligned at next step.
# We'll collect X sequences from the generator and use X as targets for the autoencoder.
gen = TimeseriesGenerator(
    features_scaled,
    features_scaled,
    length=sequence_length,
    batch_size=batch_size,
    shuffle=False,
)

X_batches = []
for X_batch, _ in gen:
    X_batches.append(X_batch)
X_seq = np.concatenate(X_batches, axis=0) if len(X_batches) > 0 else np.empty((0, sequence_length, len(feature_cols)))
y_seq = X_seq.copy()  # autoencoder target equals input sequence

num_sequences = X_seq.shape[0]
split_index = int(num_sequences * 0.8)
X_train, X_val = X_seq[:split_index], X_seq[split_index:]
y_train, y_val = y_seq[:split_index], y_seq[split_index:]

print(
    f"Sequences total: {X_seq.shape}, Train: {X_train.shape}, Val: {X_val.shape}, Features: {len(feature_cols)}"
)# Load FD001 data with proper column names
# Columns: engine_id, time, op_setting_1..3, sensor_1..21
columns = [
    "engine_id",
    "time",
    *[f"op_setting_{i}" for i in range(1, 4)],
    *[f"sensor_{i}" for i in range(1, 22)],
]

# Whitespace-separated, no header
fd001 = pd.read_csv(TRAIN_PATH, sep=r"\s+", header=None, names=columns)
print(fd001.shape)
fd001.head()# Download and extract CMAPSS FD001 via gdown
import gdown
import zipfile

DATA_DIR = Path("cmapss_data")
DATA_DIR.mkdir(exist_ok=True)

GOOGLE_DRIVE_URL = "https://drive.google.com/uc?id=1cyafqY7LIbQhCbOE1P9vE0ghq04n2Vsp"
ZIP_PATH = DATA_DIR / "CMAPSS_FD001.zip"
EXTRACT_DIR = DATA_DIR

if not ZIP_PATH.exists():
    print("Downloading dataset zip...")
    gdown.download(GOOGLE_DRIVE_URL, str(ZIP_PATH), quiet=False)
else:
    print("Zip already exists, skipping download.")

print("Extracting...")
with zipfile.ZipFile(ZIP_PATH, "r") as zf:
    zf.extractall(str(EXTRACT_DIR))
print(f"Extracted to: {EXTRACT_DIR.resolve()}")

# Locate train file
possible_train = list(EXTRACT_DIR.rglob("train_FD001.txt"))
if not possible_train:
    raise FileNotFoundError("Could not find train_FD001.txt after extraction.")
TRAIN_PATH = possible_train[0]
print(f"Found training file: {TRAIN_PATH}")# Imports and setup
import os
import random
import warnings
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf

# Reproducibility
np.random.seed(42)
random.seed(42)
tf.random.set_seed(42)

# Plotting style
sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (10, 4)
warnings.filterwarnings("ignore")# Install dependencies (Colab)
pip install -q gdown numpy pandas matplotlib seaborn scikit-learn tensorflow keras## Predictive Maintenance with NASA CMAPSS FD001 (Colab-Ready)

This notebook demonstrates a simple LSTM Autoencoder approach for anomaly detection on the NASA CMAPSS FD001 dataset. It covers:
- Installing dependencies (gdown, numpy, pandas, matplotlib, seaborn, scikit-learn, tensorflow, keras)
- Downloading and extracting the dataset from Google Drive via gdown
- Loading and normalizing the data
- Creating fixed-length sequences using Keras TimeseriesGenerator for one engine
- Training an LSTM Autoencoder to learn normal behavior
- Computing reconstruction errors and flagging anomalies via a percentile threshold

You can extend this to all engines and other models (e.g., XGBoost) later.